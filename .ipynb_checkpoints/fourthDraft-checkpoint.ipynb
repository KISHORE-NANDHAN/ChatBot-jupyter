{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039ff800",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: pymongo in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pyyaml in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\users\\kishore nandhan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 217.9 kB/s eta 0:00:59\n",
      "     --------------------------------------- 0.0/12.8 MB 326.8 kB/s eta 0:00:40\n",
      "     --------------------------------------- 0.1/12.8 MB 871.5 kB/s eta 0:00:15\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.2/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.5/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.7/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.8/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.0/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.1/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.3/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.7/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.8/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.1/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.7/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.0/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.3/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.6/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy pymongo pyyaml\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e828f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting os-sys"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for os-sys from https://files.pythonhosted.org/packages/89/4a/ecf84210a9c02f4d246c10f28a50050c6934eb9257298eb6d7aeef32c38b/os_sys-2.1.4-py3-none-any.whl.metadata\n",
      "  Downloading os_sys-2.1.4-py3-none-any.whl.metadata (9.9 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install os-sys re pyyaml spacy pymongo fuzzywuzzy python-dotenv langchain-huggingface functools python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe08622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import spacy\n",
    "from pymongo import MongoClient\n",
    "from fuzzywuzzy import process\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from functools import lru_cache\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb+srv://Admin:Manager@cluster0.vths3.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client[\"CollegeDB\"]\n",
    "faculty_collection = db[\"departments\"]\n",
    "\n",
    "# Set up Hugging Face API\n",
    "HUGGINGFACE_API_KEY = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_API_KEY\n",
    "\n",
    "# Load the Mistral-7B model\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    max_length=700,  # Allows detailed responses\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load configuration file\n",
    "with open(\"search_config.yaml\", \"r\") as file:\n",
    "    config_data = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a1a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] User Query: Who is Veeraiah in computer science and engineering?\n",
      "[DEBUG] Lemmatized Query: ['who_is', 'veeraiah', 'in', 'cse', 'science', 'and', 'engineering']\n",
      "[DEBUG] Checking intents with query: ['who_is', 'veeraiah', 'in', 'cse', 'science', 'and', 'engineering']\n",
      "[DEBUG] No intent matched.\n",
      "[DEBUG] No valid intent detected. Showing default response.\n",
      "Sorry, I couldn't understand your request.\n"
     ]
    }
   ],
   "source": [
    "# Mapping for department name variations\n",
    "department_mapping = {\n",
    "    \"computer science and engineering\": \"cse\",\n",
    "    \"information technology\": \"it\",\n",
    "    \"electronics and communication engineering\": \"ece\",\n",
    "    \"mechanical engineering\": \"mech\",\n",
    "    \"cse\": \"cse\",  # Support shorthand as well\n",
    "    \"it\": \"it\",\n",
    "    \"ece\": \"ece\",\n",
    "    \"mech\": \"mech\"\n",
    "}\n",
    "\n",
    "def lemmatize_query(query):\n",
    "    phrase_replacements = {\n",
    "        \"who is\": \"who_is\",\n",
    "        \"tell me about\": \"tell_me_about\",\n",
    "        \"what is\": \"what_is\"\n",
    "    }\n",
    "    \n",
    "    # Title abbreviations - converting \"Dr.\" to \"Dr\"\n",
    "    query = re.sub(r'\\b(Dr\\.|Prof\\.)\\s', '', query)  # Remove titles\n",
    "    \n",
    "    # Remove trailing dots in abbreviations (e.g., D. -> D)\n",
    "    query = re.sub(r'\\b([A-Z])\\.\\s', r'\\1 ', query)\n",
    "\n",
    "    for phrase, replacement in phrase_replacements.items():\n",
    "        query = query.lower().replace(phrase, replacement)\n",
    "\n",
    "    doc = nlp(query)\n",
    "    lemmatized = [token.lemma_ for token in doc if not token.is_punct]\n",
    "\n",
    "    # Map full-form departments to shorthand\n",
    "    lemmatized = [\n",
    "        department_mapping.get(\" \".join(lemmatized[i:i+4]), token)  \n",
    "        for i, token in enumerate(lemmatized)\n",
    "    ]\n",
    "\n",
    "    print(f\"[DEBUG] Lemmatized Query: {lemmatized}\")\n",
    "    return lemmatized\n",
    "\n",
    "def identify_intent_and_entities(lemmatized_query):\n",
    "    print(f\"[DEBUG] Checking intents with query: {lemmatized_query}\")\n",
    "\n",
    "    for entry in config_data:\n",
    "        if any(keyword in ' '.join(lemmatized_query) for keyword in entry[\"keywords\"]):\n",
    "            entities = {}\n",
    "\n",
    "            faculty_name = \" \".join([token for token in lemmatized_query if token not in [\"who\", \"is\", \"in\", \"and\"]])\n",
    "            department = process.extractOne(faculty_name, [\"CSE\", \"IT\", \"ECE\", \"MECH\", \"Civil\"], score_cutoff=60)\n",
    "\n",
    "            entities[\"faculty_name\"] = faculty_name.strip()\n",
    "            entities[\"department\"] = department[0] if department else \"\"\n",
    "\n",
    "            print(f\"[DEBUG] Extracted Entities: {entities}\")\n",
    "            return entry, entities\n",
    "\n",
    "    print(\"[DEBUG] No intent matched.\")\n",
    "    return None, {}\n",
    "\n",
    "\n",
    "def fetch_data(intent_data, entities):\n",
    "    print(f\"[DEBUG] Entities for Search: {entities}\")\n",
    "    collection = db[intent_data[\"collection\"]]\n",
    "\n",
    "    faculty_name = entities.get(\"faculty_name\")\n",
    "    department = entities.get(\"department\")\n",
    "\n",
    "    query_filter = {}\n",
    "    if department:\n",
    "        query_filter[\"name\"] = {\"$regex\": department, \"$options\": \"i\"}\n",
    "\n",
    "    if faculty_name:\n",
    "        # Fuzzy search for faculty name\n",
    "        potential_faculty = [faculty['name'] for department in collection.find({}, {\"faculty.name\": 1, \"_id\": 0}) for faculty in department.get(\"faculty\", [])]\n",
    "        best_match, score = process.extractOne(faculty_name, potential_faculty, score_cutoff=70)\n",
    "\n",
    "        if best_match:\n",
    "            query_filter[\"faculty.name\"] = best_match\n",
    "\n",
    "        result = collection.aggregate([\n",
    "            {\"$match\": query_filter},     \n",
    "            {\"$unwind\": \"$faculty\"},\n",
    "            {\"$match\": {\"faculty.name\": best_match}},\n",
    "            {\"$project\": {\"faculty\": 1, \"_id\": 0}}\n",
    "        ])\n",
    "    else:\n",
    "        result = collection.find(query_filter, {\"_id\": 0})\n",
    "\n",
    "    faculty_info = list(result)\n",
    "    return faculty_info[0][\"faculty\"] if faculty_info else \"No matching records found.\"\n",
    "\n",
    "\n",
    "# Generate Response\n",
    "def generate_response(query):\n",
    "    print(f\"[DEBUG] Generating response for query: {query}\")\n",
    "    faculty_info = fetch_faculty_info(query)\n",
    "\n",
    "    if isinstance(faculty_info, str):  \n",
    "        print(f\"[DEBUG] Faculty Info Not Found: {faculty_info}\")\n",
    "        return faculty_info\n",
    "\n",
    "    structured_text = (\n",
    "        f\"Faculty Name: {faculty_info.get('name', 'N/A')}\\n\"\n",
    "        f\"Designation: {faculty_info.get('designation', 'N/A')}\\n\"\n",
    "        f\"Department: {faculty_info.get('department', 'N/A')}\\n\"\n",
    "        f\"Experience: {faculty_info.get('experience', 'N/A')} years\\n\"\n",
    "        f\"Email: {', '.join(faculty_info.get('email', ['N/A']))}\\n\"\n",
    "        f\"Research Areas: {', '.join(faculty_info.get('research_areas', []))}\\n\"\n",
    "        f\"Publications: {faculty_info['publications'].get('journals', 0)} Journals, \"\n",
    "        f\"{faculty_info['publications'].get('conferences', 0)} Conferences\\n\"\n",
    "        f\"Professional Memberships: {', '.join(faculty_info.get('professional_memberships', []))}\"\n",
    "    )\n",
    "\n",
    "    # Mistral-7B-Instruct prompt\n",
    "    prompt = (\n",
    "        f\"Convert the following structured faculty information into a human-like response:\\n\\n\"\n",
    "        f\"\\\"\\\"\\\"\\n{structured_text}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        f\"Generate a concise yet detailed response as a human would say.\"\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"[DEBUG] Generated Response: {response.strip()}\")\n",
    "    return response.strip()\n",
    "\n",
    "# Truncate Output\n",
    "def truncate_output(result, max_length):\n",
    "    result_str = str(result)\n",
    "    print(f\"[DEBUG] Truncated Result (Length {max_length}): {result_str[:max_length]}\")\n",
    "    return result_str[:max_length] + (\"...\" if len(result_str) > max_length else \"\")\n",
    "\n",
    "# Main Function\n",
    "def process_query(user_query):\n",
    "    print(f\"[DEBUG] User Query: {user_query}\")\n",
    "    \n",
    "    lemmatized_query = lemmatize_query(user_query)\n",
    "    intent_data, entities = identify_intent_and_entities(lemmatized_query)\n",
    "\n",
    "    if intent_data:\n",
    "        result = fetch_data(intent_data, entities)\n",
    "        return truncate_output(result, intent_data[\"max_output_length\"])\n",
    "    \n",
    "    print(\"[DEBUG] No valid intent detected. Showing default response.\")\n",
    "    return \"Sorry, I couldn't understand your request.\"\n",
    "\n",
    "# Example usage\n",
    "user_input = \"Who is Veeraiah in computer science and engineering?\"\n",
    "print(process_query(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92358e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36954d08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generating response for query: Who is sudhakar in computer science and engineering?\n",
      "[DEBUG] Lemmatized Query: ['who_is', 'sudhakar', 'in', 'computer', 'science', 'and', 'engineering']\n",
      "[DEBUG] Checking intents with query: ['who_is', 'sudhakar', 'in', 'computer', 'science', 'and', 'engineering']\n",
      "[DEBUG] Extracted Entities: {'faculty_name': 'sudhakar', 'department': 'computer science and engineering'}\n",
      "[DEBUG] Entities for Search: {'faculty_name': 'sudhakar', 'department': 'computer science and engineering'}\n",
      "[DEBUG] Aggregation Pipeline Structure:\n",
      "{'$match': {'name': {'$regex': 'computer science and engineering', '$options': 'i'}}}\n",
      "{'$unwind': {'path': '$faculty', 'preserveNullAndEmptyArrays': True}}\n",
      "{'$match': {'faculty.name': {'$regex': 'sudhakar', '$options': 'i'}}}\n",
      "{'$project': {'faculty.name': 1, 'faculty.designation': 1, 'faculty.department': 1, 'faculty.image_url': 1, 'faculty.pdf_url': 1, 'faculty.experience': 1, 'faculty.email': 1, 'faculty.phone': 1, 'faculty.education': 1, 'faculty.teaching_subjects': 1, 'faculty.research_areas': 1, 'faculty.publications': 1, 'faculty.professional_memberships': 1, 'faculty.achievements_and_awards': 1, 'faculty.roles': 1, '_id': 0}}\n",
      "[DEBUG] Retrieved Faculty Info: {'faculty': {'name': 'Mr. A. Sudhakar', 'designation': 'Sr. Assistant Professor', 'department': 'CSE', 'image_url': 'https://lbrce.ac.in/faculty/assets/images/profiles/cse/26_sudhakar.jpg', 'pdf_url': 'https://www.lbrce.ac.in/faculty/profiles/cse/T597_SUDHAKAR_PROFILE.pdf', 'research_areas': ['Computer Networks', 'Network Security'], 'email': ['asudhakar@Lbrce.ac.in', 'sudhakar.atchala@gmail.com'], 'phone': ['+91 8801320910'], 'experience': '14+ Years', 'education': {'mtech': 'Computer Science and Engineering in Jawaharlal Nehru Technological University Kakinada, Kakinada in 2012.', 'btech': 'Information Technology in Andhra University Visakhapatnam, Visakhapatnam in 2005'}, 'teaching_subjects': ['Data Structures', 'Programming for Problem Solving using C', 'OOP through Java', 'Discrete Mathematical Structures'], 'publications': {'journals': 7, 'conferences': 1, 'patents': 0}, 'professional_memberships': ['CSI', 'IAENG', 'IEEE'], 'achievements_and_awards': ['Question Paper Setter to various Autonomous Colleges and Deemed Universities'], 'roles': ['Discipline committee member in CSE Dept., LBRCE Mylavaram', 'Counselor for 20 students to monitor their performance']}}\n",
      "[DEBUG] Structured Response:\n",
      "Faculty Name: Mr. A. Sudhakar\n",
      "Designation: Sr. Assistant Professor\n",
      "Department: CSE\n",
      "Experience: 14+ Years years\n",
      "Email: asudhakar@Lbrce.ac.in, sudhakar.atchala@gmail.com\n",
      "Phone: +91 8801320910\n",
      "Education: mtech,btech\n",
      "Teaching Subjects: Data Structures, Programming for Problem Solving using C, OOP through Java, Discrete Mathematical Structures\n",
      "Research Areas: Computer Networks, Network Security\n",
      "Publications: 7 Journals, 1 Conferences\n",
      "Professional Memberships: CSI, IAENG, IEEE\n",
      "Achievements and Awards: Question Paper Setter to various Autonomous Colleges and Deemed Universities\n",
      "Roles: Discipline committee member in CSE Dept., LBRCE Mylavaram, Counselor for 20 students to monitor their performance\n",
      "Image URL: https://lbrce.ac.in/faculty/assets/images/profiles/cse/26_sudhakar.jpg\n",
      "Profile PDF: https://www.lbrce.ac.in/faculty/profiles/cse/T597_SUDHAKAR_PROFILE.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\KISHORE NANDHAN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, \"Hello, I'm Mr. A. Sudhakar, a Sr. Assistant Professor in the Department of Computer Science and Engineering (CSE) at LBRCET, with over 14 years of experience in the field. My areas of expertise include Data Structures, Programming for Problem Solving using C, Object-Oriented Programming through Java, and Discrete Mathematical Structures. In research, I focus on Computer Networks and Network Security. I have published 8 articles in journals and 1 in conferences. I'm a proud member of the Computer Society of India (CSI), IAENG, and IEEE. I've also had the honor of setting question papers for various autonomous colleges and deemed universities. In addition to my teaching and research responsibilities, I serve as a discipline committee member in the CSE Department at LBRCET, a counselor for 20 students, and a member of the Mylavaram campus. You can reach me at asudhakar@Lbrce.ac.in or sudhakar.atchala@gmail.com, or give me a call at +91 8801320910. Here's my profile PDF for more details: <https://www.lbrce.ac.in/faculty/profiles/cse/T597_SUDHAKAR_PROFILE.pdf>. And, here's a photo of me: <https://lbrce.ac.in/faculty/assets/images/profiles/cse/26_sudhakar.jpg>\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import spacy\n",
    "from pymongo import MongoClient\n",
    "from fuzzywuzzy import process\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# MongoDB Connection\n",
    "client = MongoClient(\"mongodb+srv://Admin:Manager@cluster0.vths3.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client[\"CollegeDB\"]\n",
    "faculty_collection = db[\"departments\"]\n",
    "\n",
    "# Load Hugging Face Model\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    max_length=700,\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# Load NLP Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Search Configuration\n",
    "with open(\"search_config.yaml\", \"r\") as file:\n",
    "    config_data = yaml.safe_load(file)\n",
    "\n",
    "# Department Name Mapping\n",
    "department_mapping = {\n",
    "    \"computer science and engineering\": \"CSE\",\n",
    "    \"information technology\": \"IT\",\n",
    "    \"electronics and communication engineering\": \"ECE\",\n",
    "    \"mechanical engineering\": \"MECH\",\n",
    "    \"cse\": \"CSE\",\n",
    "    \"it\": \"IT\",\n",
    "    \"ece\": \"ECE\",\n",
    "    \"mech\": \"MECH\",\n",
    "    \"cse\":\"computer science and engineering\"\n",
    "}\n",
    "\n",
    "# Function to Process Query and Lemmatize Text\n",
    "def lemmatize_query(query):\n",
    "    phrase_replacements = {\n",
    "        \"who is\": \"who_is\",\n",
    "        \"tell me about\": \"tell_me_about\",\n",
    "        \"what is\": \"what_is\"\n",
    "    }\n",
    "    \n",
    "    # Normalize department names before tokenization\n",
    "    for full_form, short_form in department_mapping.items():\n",
    "        if full_form in query.lower():\n",
    "            query = query.lower().replace(full_form, short_form.upper())\n",
    "\n",
    "    # Remove academic titles\n",
    "    query = re.sub(r'\\b(Dr\\.|Prof\\.)\\s', '', query)\n",
    "\n",
    "    # Preserve search_config.yaml phrases\n",
    "    for phrase, replacement in phrase_replacements.items():\n",
    "        query = query.lower().replace(phrase, replacement)\n",
    "\n",
    "    doc = nlp(query)\n",
    "    lemmatized = [token.lemma_ for token in doc if not token.is_punct]\n",
    "\n",
    "    print(f\"[DEBUG] Lemmatized Query: {lemmatized}\")\n",
    "    return lemmatized\n",
    "\n",
    "# Function to Identify Intent and Extract Entities\n",
    "# Function to Identify Intent and Extract Entities\n",
    "def identify_intent_and_entities(lemmatized_query):\n",
    "    print(f\"[DEBUG] Checking intents with query: {lemmatized_query}\")\n",
    "\n",
    "    query_text = \" \".join(lemmatized_query)\n",
    "\n",
    "    for entry in config_data:\n",
    "        for keyword in entry[\"keywords\"]:\n",
    "            if keyword.lower().replace(\" \", \"_\") in query_text:\n",
    "                entities = {}\n",
    "\n",
    "                # Extract Faculty Name (Remove Stop Words & Department)\n",
    "                words_to_exclude = [\"who_is\", \"tell_me_about\", \"what_is\", \"in\", \"and\"]\n",
    "\n",
    "                # Identify Department Using Fuzzy Matching\n",
    "                department = None\n",
    "                for full_form in department_mapping.keys():\n",
    "                    if full_form.lower() in query_text.lower():\n",
    "                        department = full_form\n",
    "                        query_text = query_text.replace(full_form.lower(), \"\")  # Remove department from query\n",
    "\n",
    "                # Handle short forms like \"CSE\", \"IT\", etc.\n",
    "                for short_form, full_name in department_mapping.items():\n",
    "                    if short_form.lower() in query_text.lower():\n",
    "                        department = full_name\n",
    "                        query_text = query_text.replace(short_form.lower(), \"\")  # Remove short form from query\n",
    "\n",
    "                # Extract Faculty Name (Remaining text after department removal)\n",
    "                extracted_words = [token for token in query_text.split() if token not in words_to_exclude]\n",
    "                faculty_name = \" \".join(extracted_words)\n",
    "\n",
    "                entities[\"faculty_name\"] = faculty_name.strip()\n",
    "                entities[\"department\"] = department if department else \"\"\n",
    "\n",
    "                print(f\"[DEBUG] Extracted Entities: {entities}\")\n",
    "                return entry, entities\n",
    "\n",
    "    print(\"[DEBUG] No intent matched.\")\n",
    "    return None, {}\n",
    "def fetch_data(intent_data, entities):\n",
    "    print(f\"[DEBUG] Entities for Search: {entities}\")\n",
    "    collection = db[intent_data[\"collection\"]]\n",
    "\n",
    "    faculty_name = entities.get(\"faculty_name\")\n",
    "    department = entities.get(\"department\")\n",
    "\n",
    "    pipeline = [\n",
    "        # 1️⃣ Match the department (allow optional \"(CSE)\")\n",
    "        {\"$match\": {\"name\": {\"$regex\": department, \"$options\": \"i\"}}},\n",
    "\n",
    "        # 2️⃣ Unwind faculty while preserving empty arrays\n",
    "        {\"$unwind\": {\"path\": \"$faculty\", \"preserveNullAndEmptyArrays\": True}},\n",
    "\n",
    "        # 3️⃣ Match faculty name (flexible for punctuation and spacing)\n",
    "        {\"$match\": {\"faculty.name\": {\"$regex\": faculty_name, \"$options\": \"i\"}}},\n",
    "\n",
    "        # 4️⃣ Select only required fields\n",
    "        {\"$project\": {\n",
    "            \"faculty.name\": 1,\n",
    "            \"faculty.designation\": 1,\n",
    "            \"faculty.department\": 1,\n",
    "            \"faculty.image_url\" : 1,\n",
    "            \"faculty.pdf_url\" : 1,\n",
    "            \"faculty.experience\": 1,\n",
    "            \"faculty.email\": 1,\n",
    "            \"faculty.phone\": 1,\n",
    "            \"faculty.education\" : 1,\n",
    "            \"faculty.teaching_subjects\":1,\n",
    "            \"faculty.research_areas\": 1,\n",
    "            \"faculty.publications\": 1,\n",
    "            \"faculty.professional_memberships\": 1,\n",
    "            \"faculty.achievements_and_awards\":1,\n",
    "            \"faculty.roles\": 1,\n",
    "            \"_id\": 0\n",
    "        }}\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"[DEBUG] Aggregation Pipeline Structure:\")\n",
    "    for stage in pipeline:\n",
    "        print(stage)\n",
    "\n",
    "    result = list(collection.aggregate(pipeline))\n",
    "\n",
    "    if result:\n",
    "        print(f\"[DEBUG] Retrieved Faculty Info: {result[0]}\")\n",
    "        return result[0]\n",
    "    else:\n",
    "        print(\"[DEBUG] No matching records found.\")\n",
    "        return \"No matching records found.\"\n",
    "\n",
    "# Generate Response\n",
    "def generate_response(query):\n",
    "    print(f\"[DEBUG] Generating response for query: {query}\")\n",
    "    \n",
    "    lemmatized_query = lemmatize_query(query)\n",
    "    intent_data, entities = identify_intent_and_entities(lemmatized_query)\n",
    "\n",
    "    if not intent_data:\n",
    "        return \"Sorry, I couldn't understand your request.\"\n",
    "\n",
    "    faculty_data = fetch_data(intent_data, entities)\n",
    "\n",
    "    if isinstance(faculty_data, str):  \n",
    "        return faculty_data  # If no record found, return error message\n",
    "    \n",
    "    # ✅ Fix: Extract faculty details correctly\n",
    "    faculty_info = faculty_data.get(\"faculty\", {})\n",
    "\n",
    "    structured_text = (\n",
    "        f\"Faculty Name: {faculty_info.get('name', 'N/A')}\\n\"\n",
    "        f\"Designation: {faculty_info.get('designation', 'N/A')}\\n\"\n",
    "        f\"Department: {faculty_info.get('department', 'N/A')}\\n\"\n",
    "        f\"Experience: {faculty_info.get('experience', 'N/A')} years\\n\"\n",
    "        f\"Email: {', '.join(faculty_info.get('email', ['N/A']))}\\n\"\n",
    "        f\"Phone: {', '.join(faculty_info.get('phone', ['N/A']))}\\n\"\n",
    "        f\"Education: {','.join(faculty_info.get('education', {}))}\\n\"\n",
    "        f\"Teaching Subjects: {', '.join(faculty_info.get('teaching_subjects', ['N/A']))}\\n\"\n",
    "        f\"Research Areas: {', '.join(faculty_info.get('research_areas', ['N/A']))}\\n\"\n",
    "        f\"Publications: {faculty_info.get('publications', {}).get('journals', 0)} Journals, \"\n",
    "        f\"{faculty_info.get('publications', {}).get('conferences', 0)} Conferences\\n\"\n",
    "        f\"Professional Memberships: {', '.join(faculty_info.get('professional_memberships', ['N/A']))}\\n\"\n",
    "        f\"Achievements and Awards: {', '.join(faculty_info.get('achievements_and_awards', ['N/A']))}\\n\"\n",
    "        f\"Roles: {', '.join(faculty_info.get('roles', ['N/A']))}\\n\"\n",
    "        f\"Image URL: {faculty_info.get('image_url', 'N/A')}\\n\"\n",
    "        f\"Profile PDF: {faculty_info.get('pdf_url', 'N/A')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"[DEBUG] Structured Response:\\n{structured_text}\")\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Convert the following structured faculty information into a human-like response:\\n\\n\"\n",
    "        f\"\\\"\\\"\\\"\\n{structured_text}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        f\"Generate a concise yet detailed response as a human would say.\"\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "# Example Usage\n",
    "user_input = \"Who is sudhakar in computer science and engineering?\"\n",
    "print(generate_response(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692eee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9c8ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query Received: Who is Dr. Veeraiah in Computer science and engineering?\n",
      "[DEBUG] MongoDB Aggregation Pipeline: [{'$match': {'department': {'$in': ['CSE']}}}, {'$unwind': {'path': '$faculty', 'preserveNullAndEmptyArrays': True}}, {'$match': {'faculty.name': {'$regex': 'Dr. Veeraiah', '$options': 'i'}}}, {'$project': {'faculty.name': 1, 'faculty.designation': 1, 'faculty.department': 1, 'faculty.experience': 1, 'faculty.email': 1, 'faculty.phone': 1, 'faculty.education': 1, 'faculty.teaching_subjects': 1, 'faculty.research_areas': 1, 'faculty.publications': 1, 'faculty.professional_memberships': 1, 'faculty.achievements_and_awards': 1, 'faculty.roles': 1, '_id': 0}}]\n",
      "No matching records found.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pymongo\n",
    "\n",
    "# Load YAML file\n",
    "def load_yaml(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://Admin:Manager@cluster0.vths3.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client[\"CollegeDB\"]\n",
    "\n",
    "# Function to build MongoDB pipeline dynamically\n",
    "def build_pipeline(mongodb_query, entities):\n",
    "    pipeline = []\n",
    "    for stage in mongodb_query:\n",
    "        stage_str = str(stage)\n",
    "        for key, value in entities.items():\n",
    "            stage_str = stage_str.replace(f\"<{key}>\", value)\n",
    "        pipeline.append(eval(stage_str))  # Convert back to dict\n",
    "    return pipeline\n",
    "\n",
    "# Generate response dynamically\n",
    "def generate_response(query, yaml_data):\n",
    "    print(f\"[DEBUG] Query Received: {query}\")\n",
    "    \n",
    "    # Find matching intent\n",
    "    for intent in yaml_data[\"intents\"]:\n",
    "        if any(keyword in query.lower() for keyword in intent[\"keywords\"]):\n",
    "            intent_data = intent\n",
    "            break\n",
    "    else:\n",
    "        return \"Sorry, I couldn't understand your request.\"\n",
    "\n",
    "    # Extract entities from the query (dummy entity extraction)\n",
    "    entities = {\"faculty_name\": \"Dr. Veeraiah\", \"department\": \"CSE\"}\n",
    "\n",
    "    # Build MongoDB Query\n",
    "    pipeline = build_pipeline(intent_data.get(\"mongodb_query\", []), entities)\n",
    "    collection = db[\"departments\"] if \"faculty_info\" in intent_data[\"intent\"] else db[\"admissions\"]\n",
    "\n",
    "    print(\"[DEBUG] MongoDB Aggregation Pipeline:\", pipeline)\n",
    "    result = list(collection.aggregate(pipeline))\n",
    "\n",
    "    if not result:\n",
    "        return \"No matching records found.\"\n",
    "\n",
    "    # Format response using the YAML template\n",
    "    response_template = intent_data.get(\"response_template\", \"\")\n",
    "    response_text = response_template.format(**result[0])  # Fill placeholders with DB values\n",
    "\n",
    "    # Generate LLM Prompt\n",
    "    llm_prompt = intent_data.get(\"llm_prompt\", \"\").replace(\"{response_text}\", response_text)\n",
    "\n",
    "    print(f\"[DEBUG] LLM Prompt: {llm_prompt}\")\n",
    "    # Simulated LLM response\n",
    "    response = f\"LLM Response: {response_text}\"\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "# Load YAML data\n",
    "yaml_data = load_yaml(\"search_config_modified.yaml\")\n",
    "\n",
    "# Example Usage\n",
    "user_input = \"Who is Dr. Veeraiah in Computer science and engineering?\"\n",
    "print(generate_response(user_input, yaml_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce065554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a64752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
